{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832cadb-bf4c-49d5-a4e6-d9ec3f8a86f5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95c46a-1eaf-485b-a40f-dad98ba2ce8e",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e106b5-b61e-4a43-80e8-776ee8c40baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "models = {\n",
    "    \"GPT-4o mini\": \"gpt-4o-mini-2024-07-18\",\n",
    "    \"Claude 3.5 Haiku\": \"claude-3-5-haiku-20241022\"\n",
    "}\n",
    "\n",
    "# Clients\n",
    "openai_client = OpenAI()\n",
    "anthropic_client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebfb3a-283f-4ad8-8211-5ddd3f519685",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d012e-93e7-4d88-a501-94d26b596e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a very serious assistant that helps customers with their technical questions. \\\n",
    "You are given a question to which you must respond with an easy to understand and concise explanation, \\\n",
    "without requiring previous knowledge on the topic from the customer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21352a32-a5e8-422e-89ad-b61d6cbfe81c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f37b2-ca33-4188-b8af-4b2cf4ccebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history, model):\n",
    "    # Remove extra keys ('metadata' and 'options') from history messages as Anthropic does not like them\n",
    "    cleaned_history = [{\"role\": message[\"role\"], \"content\": message[\"content\"]} for message in history]\n",
    "\n",
    "    response = \"\"\n",
    "    if model == \"GPT-4o mini\":\n",
    "        messages = [{\"role\": \"system\", \"content\": system_message}] + cleaned_history + [{\"role\": \"user\", \"content\": message}]\n",
    "        stream = openai_client.chat.completions.create(model=models[model], messages=messages, stream=True)\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            yield response\n",
    "    elif model == \"Claude 3.5 Haiku\":\n",
    "        messages = cleaned_history + [{\"role\": \"user\", \"content\": message}]\n",
    "        result = anthropic_client.messages.stream(model=models[model], max_tokens=1000, system=system_message, messages=messages)\n",
    "        with result as stream:\n",
    "            for chunk in stream.text_stream:\n",
    "                response += chunk or \"\"\n",
    "                yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc934e-5ff6-4107-85de-f8cba281fb5a",
   "metadata": {},
   "source": [
    "## User Interface (UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218eefc-128a-48ea-9e5b-1a72c32d4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    model = gr.Dropdown(models, label=\"Model\")\n",
    "\n",
    "    gr.ChatInterface(\n",
    "        fn=chat,\n",
    "        additional_inputs=[model],\n",
    "        type=\"messages\",\n",
    "        # save_history=True,\n",
    "    )\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1fa54-7b77-45b9-8e78-c5da065921a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
